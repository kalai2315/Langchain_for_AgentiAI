{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBrDOJNeg+5OJ4bwsi1Zd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalai2315/Langchain_for_AgentiAI/blob/main/Conversation_Chains_and_Memory__with_LCEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7ZISkBCfdJT",
        "outputId": "870e5cf1-40d8-4911-c8db-a56bbbcdb9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet langchain langchain-google-genai langchain-community langchain-chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv-haGGMgSZK",
        "outputId": "7fe678f9-60b9-472a-8eed-f31e81f69c01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "4_E_kL8ngkNK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "jR24ykgVhFxa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_txt = \"\"\"{query}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_txt)"
      ],
      "metadata": {
        "id": "D7yoAcFcheXq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = prompt | llm"
      ],
      "metadata": {
        "id": "4k-wu67phxzC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({\"query\": \"What are the first four colours in rainbow?\"})"
      ],
      "metadata": {
        "id": "OsXPsP6aiqEg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNGxdaxUi-Ge",
        "outputId": "d6d09c83-967f-4b17-831a-9ce0a272fbb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colours in a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({\"query\": \"and the other three?\"})"
      ],
      "metadata": {
        "id": "LkM3UqQ5jHep"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojxOrpFpjMS3",
        "outputId": "368c3c56-2717-49d1-846a-fe2df94dcec5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide me with the context or information about the \"other three\" you're referring to.  I need more information to answer your question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation chain with ConversationBufferMemory"
      ],
      "metadata": {
        "id": "BRrD4lkOjeMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
      ],
      "metadata": {
        "id": "VvBLf5TDjSim"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. System prompt\n",
        "SYS_PROMPT = \"\"\" Act as a helpful assistant and give brief answers\"\"\"\n",
        "\n",
        "# 2. Prompt with memory placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Initialize memory\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Joh44PnyeT",
        "outputId": "ec9a6673-57f4-4b0b-9e6f-02d7fab28e15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-1561153591.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFlNfWfAoq-7",
        "outputId": "6d53f4e1-683b-4a39-f618-7e9dff6f95be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Get memory messages using the entire input dictionary\n",
        "def get_memory_messages(inputs: dict) -> list:\n",
        "    return memory.load_memory_variables(inputs)[\"history\"]\n",
        "\n",
        "get_memory_messages(\"what are the first four colours in rainbow?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfNcwaqsov6o",
        "outputId": "72bd829f-cd73-4a95-d9cf-dc2ef1641836"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RunnableLambda(get_memory_messages).invoke({\"query\": \"what are the first four colours in rainbow?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeDeHFsqhYz",
        "outputId": "1def6246-ec71-4650-c12e-ea556291f0ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RunnablePassthrough.assign(\n",
        "    history = RunnableLambda(get_memory_messages)\n",
        ").invoke({\"query\": \"what are the first four colours in rainbow?\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HGdmHd7rl0v",
        "outputId": "e572332f-8118-4578-8492-df9be45c8231"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what are the first four colours in rainbow?', 'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_memory_messages(query):\n",
        "  return memory.load_memory_variables(query)[\"history\"]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "FLEiiDt1sFsT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"what are the first four colours in rainbow?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYn5kS1wtI_3",
        "outputId": "a17c8f29-a6ab-493a-9e2e-2addd7e0cc78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Red, orange, yellow, green.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash-latest', 'safety_ratings': []}, id='run--fb9dd154-0bfb-4ef6-9b16-c5e7d47cd81e-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zadS2LXj6AIZ",
        "outputId": "ece62865-7c63-480b-a4f5-de92e07474d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red, orange, yellow, green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4EiqN1M_2FO",
        "outputId": "97d97dc4-8ba9-4283-9908-63694d841c06"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(query, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "-d2n316fAB3d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qJ6YIXbAZof",
        "outputId": "e24d85f8-1102-4322-892d-2a3845164e85"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='what are the first four colours in rainbow?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Red, orange, yellow, green.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"and the other three?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFXIrOWzAfrB",
        "outputId": "9c2ac4ed-1cc9-4ea1-d4cf-39b58e533e59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue, indigo, violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-FjyOUpA3ie",
        "outputId": "c041efb6-5902-4b26-b943-3d9cb748b5f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='what are the first four colours in rainbow?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Red, orange, yellow, green.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='and the other three?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Blue, indigo, violet.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Explain AI in 3 bullet points?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq8bYJ4nA-jF",
        "outputId": "47d078eb-51f3-4197-e5e6-c5fe3c848d83"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Mimics human intelligence:** AI systems are designed to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
            "* **Learns from data:** AI algorithms improve their performance over time by analyzing vast amounts of data.\n",
            "* **Diverse applications:** AI is used across many fields, from healthcare and finance to transportation and entertainment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Now do the same for Deep learning?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_GOrPowBPtm",
        "outputId": "60183109-ea5a-469d-a4bd-1d1bf06fdb74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Subset of AI:** Deep learning is a specialized type of machine learning, a subset of AI.\n",
            "* **Artificial neural networks:** It uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.\n",
            "* **Feature extraction:**  Automatically learns complex features from raw data, eliminating the need for manual feature engineering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"What have we discussed so far?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBTxvnObBZBj",
        "outputId": "df391f31-a66b-466d-e179-fe7193c83220"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've discussed rainbows, AI, and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0OEnJuJBhxi",
        "outputId": "bd601191-67a7-44b9-f574-edd22d8f2b7f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='what are the first four colours in rainbow?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Red, orange, yellow, green.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='and the other three?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Blue, indigo, violet.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Explain AI in 3 bullet points?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='* **Mimics human intelligence:** AI systems are designed to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\\n* **Learns from data:** AI algorithms improve their performance over time by analyzing vast amounts of data.\\n* **Diverse applications:** AI is used across many fields, from healthcare and finance to transportation and entertainment.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Now do the same for Deep learning?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='* **Subset of AI:** Deep learning is a specialized type of machine learning, a subset of AI.\\n* **Artificial neural networks:** It uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.\\n* **Feature extraction:**  Automatically learns complex features from raw data, eliminating the need for manual feature engineering.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What have we discussed so far?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"We've discussed rainbows, AI, and deep learning.\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation chain with ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "rboAY4QHB3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
      ],
      "metadata": {
        "id": "jogd42WlBpiM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. System prompt\n",
        "SYS_PROMPT = \"\"\" Act as a helpful assistant and give brief answers\"\"\"\n",
        "\n",
        "# 2. Prompt with memory placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Initialize memory\n",
        "memory = ConversationBufferWindowMemory(return_messages=True, k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fgC1Y9YCqsI",
        "outputId": "f6bb6789-b781-4eaa-f778-b10a0187a949"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-31-720944166.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_memory_messages(query):\n",
        "  return memory.load_memory_variables(query)[\"history\"]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "AXcfu-5PC0uL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"what are the first four colours in rainbow?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHYYK3FC5iP",
        "outputId": "2d924c45-2492-4dc4-fda1-c40a336b9511"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red, orange, yellow, green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"and the other three?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV6hWaBqDRTU",
        "outputId": "9a421b34-1050-4f6a-a0cf-a22f84c44a6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue, indigo, violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"and the other three?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn0CtAONDU4W",
        "outputId": "1943b8be-0522-499b-c0be-715baf15d700"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are only six colours in a rainbow.  There aren't \"other three\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Explain AI in 3 bullet points?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii4oxwwdDcaH",
        "outputId": "84c6319b-bb33-46d8-9afc-ad8b0df8524c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Mimics human intelligence:** AI systems are designed to perform tasks that typically require human intelligence, like learning, problem-solving, and decision-making.\n",
            "* **Learns from data:** AI algorithms improve their performance by analyzing vast amounts of data and identifying patterns.\n",
            "* **Adapts and improves:**  Many AI systems can adapt to new information and refine their processes over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Now do the same for Deep learning?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRUv86o8DffV",
        "outputId": "d1462149-26c3-4a1b-d252-c079a6c4b378"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Uses artificial neural networks:** Deep learning relies on complex, layered neural networks to analyze data.\n",
            "* **Learns hierarchical features:** It automatically learns increasingly abstract features from raw data, unlike simpler machine learning models.\n",
            "* **Requires large datasets:** Deep learning models need massive amounts of data to train effectively and achieve high accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKkLLLejDjYC",
        "outputId": "e5033690-48b3-4aec-885d-fe98a4202d9a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Explain AI in 3 bullet points?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='* **Mimics human intelligence:** AI systems are designed to perform tasks that typically require human intelligence, like learning, problem-solving, and decision-making.\\n* **Learns from data:** AI algorithms improve their performance by analyzing vast amounts of data and identifying patterns.\\n* **Adapts and improves:**  Many AI systems can adapt to new information and refine their processes over time.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Now do the same for Deep learning?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='* **Uses artificial neural networks:** Deep learning relies on complex, layered neural networks to analyze data.\\n* **Learns hierarchical features:** It automatically learns increasingly abstract features from raw data, unlike simpler machine learning models.\\n* **Requires large datasets:** Deep learning models need massive amounts of data to train effectively and achieve high accuracy.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"What have we discussed so far?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB8hkM5FDzrx",
        "outputId": "26956d6c-5d12-4a8c-a8e9-c39f9f154ccd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've discussed Artificial Intelligence (AI) generally and then focused specifically on Deep Learning, highlighting their key characteristics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation chain with ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "t-CMJHyFD--v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "\n",
        "# 1. System prompt\n",
        "SYS_PROMPT = \"\"\" Act as a helpful assistant and give brief answers\"\"\"\n",
        "\n",
        "# 2. Prompt with memory placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Initialize memory\n",
        "memory = ConversationSummaryMemory(return_messages=True, llm=llm)\n",
        "\n",
        "\n",
        "def get_memory_messages(query):\n",
        "  return memory.load_memory_variables(query)[\"history\"]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BusR9_ekD5fI",
        "outputId": "dc67536d-c1ed-415b-b3ee-888d81107e57"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-2398032088.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(return_messages=True, llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Explain AI in 3 bullet points?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnAe1BTAEuCT",
        "outputId": "e89b531e-9b8b-436e-fa3a-720a3ed7fb21"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Mimicking human intelligence:** AI systems are designed to perform tasks that typically require human intelligence, like learning, problem-solving, and decision-making.\n",
            "\n",
            "* **Learning from data:** AI algorithms learn from large datasets to improve their performance over time without explicit programming.\n",
            "\n",
            "* **Applications across many fields:** AI is used in various sectors, including healthcare, finance, transportation, and entertainment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Now do the same for Deep learning?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGR6sIRtE_lY",
        "outputId": "78731e70-fe7f-4727-de20-606b1c39717c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Uses artificial neural networks with multiple layers.\n",
            "* Learns complex patterns from massive datasets.\n",
            "* Powers advanced applications like image recognition and natural language processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnuEOGGTFCfK",
        "outputId": "70f5c51a-e0a8-498a-9a6d-c85155d82d4f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='The human asks the AI to explain AI in three bullet points, which are that AI mimics human intelligence, learns from data to improve performance, and has applications across many fields.  The human then asks for a similar explanation of deep learning, which the AI describes as using artificial neural networks with multiple layers to learn complex patterns from massive datasets, powering applications like image recognition and natural language processing.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation chain with VectorStoreRetrieverMemory"
      ],
      "metadata": {
        "id": "97jZSRAWFSQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
      ],
      "metadata": {
        "id": "dxEeOamqGfPO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
      ],
      "metadata": {
        "id": "24YcrkNcFM4J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a vector database to store conversation history\n"
      ],
      "metadata": {
        "id": "HilfR6p8G7TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_db = Chroma(collection_name = \"history_db\",\n",
        "                   embedding_function = embedding_model )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM4rsKTzFFhm",
        "outputId": "f7d651bc-30fd-499e-f5e0-ce2f7095e764"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2205453753.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  chroma_db = Chroma(collection_name = \"history_db\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "\n",
        "# 1. System prompt\n",
        "SYS_PROMPT = \"\"\" Act as a helpful assistant and give brief answers\"\"\"\n",
        "\n",
        "# 2. Prompt with memory placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "W8m2krmwHseC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = chroma_db.as_retriever(search_type = \"similarity\",\n",
        "                                   search_kwargs = {\"k\": 2})\n",
        "memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkkamidKIOu_",
        "outputId": "151d67ac-311c-4ee4-e8e7-8b40d124f39c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2359598408.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_memory_messages(query):\n",
        "  return [memory.load_memory_variables(query)[\"history\"]]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "FbwTajBgI4Sk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Tell me about AI\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFpCObHfKd6q",
        "outputId": "94caf42d-1259-4c4a-cda7-b8565433a648"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) involves creating computer systems capable of performing tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.  It encompasses various techniques, including machine learning and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"What about Deep Learning?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu81R5o_Kv2P",
        "outputId": "6b49c24b-b15c-4633-eda6-a1b9b2cc62de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and extract complex patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"Tell me about the fastest animal in the world in two lines?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmeVR_22K2OC",
        "outputId": "3d7ed5e5-78cc-46c9-ce80-48433617d326"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The peregrine falcon is the fastest animal.  It can reach speeds exceeding 240 mph during its hunting stoop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\"query\": \"What about cheetah?\"}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d3m3Pq3LAPq",
        "outputId": "b9787d32-1d39-4b8c-a2b9-5b9c293edd63"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cheetah is the fastest land animal, reaching speeds up to 75 mph in short bursts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.load_memory_variables({\"query\": \"What about Machine Learning?\"})['history'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYKUMcSELTvW",
        "outputId": "88f1057b-dffc-4235-d567-f1256beeab4c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: What about Deep Learning?\n",
            "output: Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and extract complex patterns.\n",
            "query: Tell me about AI\n",
            "output: Artificial intelligence (AI) involves creating computer systems capable of performing tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.  It encompasses various techniques, including machine learning and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiUser Conversation chains with ConversationMessageHistory"
      ],
      "metadata": {
        "id": "u26lIy2JMP2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9KVHySclLL4",
        "outputId": "826d252f-7652-4930-d445-315b13da788e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n4BiE3DlY2V",
        "outputId": "6d8d0b1e-437b-4e87-9b67-800ec6842bb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.26\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: langchain-community\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "vPPoX62jL6Zr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "xR0ohMu7kHoJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{human_input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "rljjWAPymY3F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_store = {}\n",
        "\n",
        "def get_session_history(session_id):\n",
        "    if session_id not in history_store:\n",
        "        history_store[session_id] = ConversationBufferMemory( memory_key=\"history\",return_messages=True)\n",
        "    return history_store[session_id].chat_memory"
      ],
      "metadata": {
        "id": "wFr8hlKpmk4u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = (prompt_template | llm)"
      ],
      "metadata": {
        "id": "ggzfYTr4m-_p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create conversation chain which can load memory based on specific user and session id\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history=get_session_history,\n",
        "    input_messages_key=\"human_input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "iEToqOgMniC7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_llm(prompt:str, session_id:str):\n",
        "  for chunk in conversation_chain.stream({\"human_input\": prompt}, config={\"configurable\": {\"session_id\": session_id}}):\n",
        "\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "sA8kTJa0oN_3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversation chain for user1\n",
        "\n",
        "user_id = 'ali123'\n",
        "prompt = 'Hi, I am Ali, can you explain AI in 3 bullet points?'\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU_2SCaKprfZ",
        "outputId": "0f362f06-7407-4773-8578-69b3b4d6f3c3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **AI mimics human intelligence:**  AI systems are designed to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
            "\n",
            "* **AI learns from data:** AI algorithms improve their performance over time by analyzing large amounts of data and identifying patterns.  This learning process allows them to adapt and become more accurate.\n",
            "\n",
            "* **AI powers many applications:** AI is not just a single technology; it's a broad field encompassing many techniques used in various applications, from self-driving cars and medical diagnosis to personalized recommendations and language translation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Do the same for deep learning?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLEFmvUouJkg",
        "outputId": "54ad0459-d571-4e75-da28-55a7f9c5b0b5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's explore deep learning, a powerful subset of machine learning.  Instead of explicitly programming rules, deep learning uses artificial neural networks with many layers (\"deep\") to learn from data.  This allows it to automatically learn complex patterns and representations from raw data, unlike simpler machine learning models that often require manual feature engineering.\n",
            "\n",
            "Think of it like this:  imagine you're teaching a child to identify a cat.  With regular machine learning, you might tell the child to look for specific features like pointy ears, whiskers, and a furry tail.  With deep learning, you just show the child many pictures of cats and non-cats.  The deep learning model, like the child's brain, gradually learns to identify cats by itself, discovering subtle features and relationships between them that you might not even have consciously noticed.\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "* **Artificial Neural Networks (ANNs):**  Deep learning relies on ANNs, which are computing systems inspired by the structure and function of the human brain. These networks consist of interconnected nodes (neurons) organized in layers.  Deep learning models have many layers, allowing them to learn increasingly complex representations of the data as the information flows through the network.\n",
            "\n",
            "* **Layers:**  Each layer in the network performs a specific transformation on the data.  Early layers might learn simple features (like edges in an image), while deeper layers learn more abstract and complex features (like shapes or objects).\n",
            "\n",
            "* **Automatic Feature Extraction:** This is a key advantage.  Unlike traditional machine learning, deep learning algorithms automatically learn relevant features from raw data without human intervention. This eliminates the need for laborious manual feature engineering, a significant time-saver and often a source of error in simpler methods.\n",
            "\n",
            "* **Data Requirements:** Deep learning models typically require massive amounts of data to train effectively.  The more data, the better the model can learn and generalize to new, unseen data.\n",
            "\n",
            "* **Computational Resources:** Training deep learning models is computationally intensive and often requires specialized hardware like GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units) to accelerate the process.\n",
            "\n",
            "* **Types of Deep Learning Architectures:** There are various architectures tailored to different tasks:\n",
            "\n",
            "    * **Convolutional Neural Networks (CNNs):**  Excellent for image and video processing.\n",
            "    * **Recurrent Neural Networks (RNNs):**  Specialized for sequential data like text and time series.  Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs) are popular RNN variations.\n",
            "    * **Generative Adversarial Networks (GANs):** Used for generating new data samples that resemble the training data (e.g., creating realistic images).\n",
            "    * **Autoencoders:**  Used for dimensionality reduction and feature extraction.\n",
            "\n",
            "\n",
            "**Deep Learning vs. Regular Machine Learning (Key Differences):**\n",
            "\n",
            "| Feature             | Deep Learning                               | Regular Machine Learning                     |\n",
            "|----------------------|--------------------------------------------|---------------------------------------------|\n",
            "| Feature Engineering | Automatic                                   | Often manual                                |\n",
            "| Model Complexity     | Highly complex, multi-layered neural networks | Simpler models                               |\n",
            "| Data Requirements   | Large datasets                             | Can work with smaller datasets              |\n",
            "| Training Time       | Long, computationally expensive              | Relatively faster                             |\n",
            "| Interpretability     | Often considered a \"black box\"              | Generally more interpretable                 |\n",
            "\n",
            "\n",
            "Deep learning has revolutionized many fields, enabling breakthroughs in image recognition, natural language processing, speech recognition, and more.  However, its high computational cost and data requirements remain significant challenges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give a brief summary of what we have discussed?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TntcsmPeurgm",
        "outputId": "e8f61900-89b5-41bc-b691-ac3f6a804366"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've covered the fundamentals of machine learning (ML) and deep learning (DL), two key areas of artificial intelligence.  ML involves algorithms that learn from data to make predictions without explicit programming, encompassing supervised, unsupervised, and reinforcement learning methods.  DL is a more advanced type of ML using deep artificial neural networks with many layers to automatically learn complex patterns from raw data, eliminating the need for manual feature engineering.  While powerful, DL requires significantly more data and computational resources than traditional ML.  We discussed the key differences between these approaches and their applications in various fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate second user\n",
        "user_id = 'user_b456'\n",
        "prompt = 'Hello, I am Bob. Can you explain what machine learning is?'\n",
        "\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omMm5hEsqxg6",
        "outputId": "305cb7dc-6c5c-4ddf-8640-ad17a4abffc8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Bob!  Machine learning (ML) is a branch of artificial intelligence (AI) where computer systems learn from data without being explicitly programmed.  Instead of relying on hard-coded rules, ML algorithms identify patterns and relationships within data to make predictions or decisions.\n",
            "\n",
            "Imagine you want to teach a dog a new trick.  You wouldn't write down a precise set of instructions; instead, you'd show the dog what to do, reward it for correct actions, and correct it when it's wrong.  Machine learning is similar. We feed the computer lots of data, and it learns to identify patterns and make predictions based on that data.\n",
            "\n",
            "Here's a simplified breakdown:\n",
            "\n",
            "* **Data:** This is the fuel for machine learning.  It could be anything from images and text to numbers and sensor readings.  The more relevant and high-quality data you have, the better the system will learn.\n",
            "* **Algorithm:** This is the set of instructions that tells the computer how to learn from the data.  There are many different algorithms, each suited to different types of problems and data.\n",
            "* **Model:**  After the algorithm processes the data, it creates a \"model.\"  This model is essentially a representation of the patterns and relationships it has learned.  It can then use this model to make predictions on new, unseen data.\n",
            "* **Prediction/Inference:** This is the final step where the model uses what it has learned to make predictions or classifications. For example, predicting the price of a house based on its size and location, or identifying a picture as a cat or a dog.\n",
            "\n",
            "\n",
            "There are several types of machine learning:\n",
            "\n",
            "* **Supervised Learning:**  The algorithm is trained on labeled data – data where the correct answers are already known.  For example, showing the computer many pictures of cats labeled \"cat\" and dogs labeled \"dog\" so it can learn to distinguish between them.\n",
            "* **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find patterns and structures on its own. For example, grouping similar customers together based on their purchasing history.\n",
            "* **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones.  Think of a robot learning to walk – it gets rewarded for taking steps and penalized for falling.\n",
            "\n",
            "Machine learning is used in many everyday applications, including:\n",
            "\n",
            "* **Spam filters:** Identifying unwanted emails.\n",
            "* **Recommendation systems:** Suggesting movies or products you might like.\n",
            "* **Medical diagnosis:** Helping doctors diagnose diseases.\n",
            "* **Self-driving cars:**  Analyzing sensor data to navigate roads.\n",
            "\n",
            "\n",
            "Is there anything specific about machine learning you'd like to know more about?  I'm happy to explain further!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Do the same for deep learning?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYiKKSRLtgRg",
        "outputId": "b1ffca8d-5cec-48ef-a92c-7d0219e1d3d6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and extract higher-level features.  Think of it as a more sophisticated and powerful version of regular machine learning.\n",
            "\n",
            "Here's a breakdown comparing it to regular machine learning:\n",
            "\n",
            "**Regular Machine Learning:**\n",
            "\n",
            "* **Feature Engineering:** Often requires manual feature extraction.  You need to explicitly tell the algorithm what features to look for in the data (e.g., for image recognition, you might tell it to look for edges, corners, and colors).\n",
            "* **Simpler Models:** Uses simpler models with fewer parameters.\n",
            "* **Less Data Required (Generally):** Can often achieve good results with relatively smaller datasets.\n",
            "* **Faster Training (Generally):** Training is typically faster due to the simpler models.\n",
            "\n",
            "\n",
            "**Deep Learning:**\n",
            "\n",
            "* **Automatic Feature Extraction:** Automatically learns features from raw data without manual intervention.  For example, a deep learning model for image recognition can learn to identify edges, corners, and other features on its own, directly from the pixel data. This is a major advantage.\n",
            "* **Complex Models:** Uses complex, multi-layered neural networks with billions of parameters.  These networks are inspired by the structure and function of the human brain.\n",
            "* **More Data Required:** Requires significantly larger datasets to train effectively.  The more data, the better the model's performance.\n",
            "* **Slower Training:** Training can take a considerable amount of time and computational resources due to the complexity of the models.\n",
            "* **Handles High-Dimensional Data Well:** Deep learning excels at working with high-dimensional data like images, videos, and audio, where traditional machine learning methods may struggle.\n",
            "\n",
            "\n",
            "**Analogy:**\n",
            "\n",
            "Imagine you're trying to identify a bird from a picture.\n",
            "\n",
            "* **Regular machine learning:** You might tell the algorithm to look for specific features like beak shape, wingspan, and feather color.  You're providing the algorithm with explicit instructions.\n",
            "* **Deep learning:** You simply show the algorithm many pictures of birds, and it learns to identify the relevant features on its own. It might learn complex relationships between different parts of the bird's image to make accurate classifications.  It's learning from the raw data without explicit feature engineering.\n",
            "\n",
            "\n",
            "**Key Differences Summarized:**\n",
            "\n",
            "| Feature           | Regular Machine Learning       | Deep Learning                  |\n",
            "|--------------------|---------------------------------|---------------------------------|\n",
            "| Feature Engineering | Manual                        | Automatic                       |\n",
            "| Model Complexity   | Simple                         | Complex, multi-layered         |\n",
            "| Data Requirements  | Less                           | Much more                       |\n",
            "| Training Time      | Faster                          | Slower                          |\n",
            "| Data Types         | Various, but often lower-dim   | Handles high-dimensional data well |\n",
            "\n",
            "\n",
            "In short, deep learning is a more powerful but also more resource-intensive approach to machine learning that's particularly well-suited for complex tasks involving large amounts of data.  It's responsible for many recent breakthroughs in AI, particularly in areas like image recognition, natural language processing, and speech recognition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give a brief summary of what we have discussed?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5DM5J5PuGMi",
        "outputId": "ec6427df-1638-48b0-d4ce-ac3d92980c02"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've discussed the core concepts of machine learning and deep learning, highlighting their key differences and similarities.\n",
            "\n",
            "**Machine learning** is a type of artificial intelligence where computer systems learn from data without explicit programming.  They identify patterns and relationships to make predictions or decisions.  We explored supervised learning (learning from labeled data), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning through trial and error).  Machine learning is used in many applications, from spam filtering to recommendation systems.\n",
            "\n",
            "**Deep learning** is a specialized subset of machine learning that uses artificial neural networks with multiple layers to learn complex representations from raw data.  A key advantage is its ability to automatically learn features, eliminating the need for manual feature engineering required in many traditional machine learning approaches.  However, deep learning requires significantly more data and computational resources than simpler machine learning methods.  We touched upon different deep learning architectures like CNNs, RNNs, GANs, and autoencoders, each suited for different types of data and tasks.  Deep learning powers many state-of-the-art AI applications in image recognition, natural language processing, and more.\n",
            "\n",
            "In short: Machine learning is a broad field, and deep learning is a powerful, specialized technique within it, offering greater capabilities but with increased computational demands.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi User Window Based Conversation chain - SQLChatMessageHistory"
      ],
      "metadata": {
        "id": "znjXWfONvd6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm memory.db #removes memory database file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yQdb126umya",
        "outputId": "27818267-aeb3-4b3e-e532-85c075b8c5ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'memory.db': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain.memory.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "import os\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
      ],
      "metadata": {
        "id": "9qyppQEJwMyL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(\"sqlite:///memory.db\")\n",
        "\n",
        "def get_session_history_db(session_id):\n",
        "    return SQLChatMessageHistory(\n",
        "        session_id=session_id,\n",
        "        connection=engine\n",
        "    )"
      ],
      "metadata": {
        "id": "BnULym0bwXLu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "yLZtcKUHxPvS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create memory buffer window function to return last k conversation\n",
        "\n",
        "def memory_buffer_window(messages,k=2):\n",
        "  return messages[-(k+1):]"
      ],
      "metadata": {
        "id": "R19JfMWQxUsN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = (\n",
        "    RunnablePassthrough.assign(history = lambda x: memory_buffer_window(x[\"history\"])))|prompt|llm\n"
      ],
      "metadata": {
        "id": "FUnMNzgkx95s"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create conversation chain which can load memory based on specific user and session id\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history_db,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        "    )"
      ],
      "metadata": {
        "id": "9HVy3zJpXdyh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_llm(prompt:str, session_id:str):\n",
        "  for chunk in conversation_chain.stream({\"input\": prompt}, config={\"configurable\": {\"session_id\": session_id}}):\n",
        "\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "ZKsl4FQ2ZKVn"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"boby123\"\n",
        "prompt =\"which is the fastest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw87xE9GZoZZ",
        "outputId": "eabc88b0-f07b-4d81-def5-146e02e02845"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The peregrine falcon is generally considered the fastest animal.  Its hunting stoop (a high-speed dive) can reach speeds exceeding 240 mph (386 km/h).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"which is the slowest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2tOt556aQ35",
        "outputId": "b0f0dad0-e274-4925-8861-f9ffdb5fb0da"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's no single definitive answer to what the slowest animal is, as \"slowest\" can depend on how you measure it (speed over a short distance, average speed over a lifetime, etc.). However, contenders for the title often include various species of **sea stars** and **giant tortoises**.  Their movements are incredibly slow compared to most other animals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"which is the largest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH6pYNYwcsiz",
        "outputId": "a7fd4b0b-f22b-4b4d-fe54-9dfddac8df4c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest animal is the **blue whale**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"what topics have we discussed, show briefly as three bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tquuonWIcu-3",
        "outputId": "283c1fcd-b09b-48cb-ae2a-36bb6413d786"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Slowest animal\n",
            "* Largest animal\n",
            "* Summary of our conversation topics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0gOACJ0c593"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}