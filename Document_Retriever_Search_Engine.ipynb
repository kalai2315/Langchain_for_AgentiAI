{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c2c22feca8545769f9b4b99addf7933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25cda6ec52444337b87122acea111ce9",
              "IPY_MODEL_d50f59269bd14d438dcba220ce51f389",
              "IPY_MODEL_3413a0d7a6854d84b61b8e6e6e91c06d"
            ],
            "layout": "IPY_MODEL_c1ffd6dd4ab14a1998250a6c5e30b243"
          }
        },
        "25cda6ec52444337b87122acea111ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9911fde248db4c3b9aa4c00e05df2ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_30700761571e4e5182aaa3fbaa9a5011",
            "value": "config.json: 100%"
          }
        },
        "d50f59269bd14d438dcba220ce51f389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efdffc6e0366476799f85a7ba3a8d022",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b38f5c97555749a7a49236bf774e76b5",
            "value": 801
          }
        },
        "3413a0d7a6854d84b61b8e6e6e91c06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0347f23930aa45668b6ee328a7361443",
            "placeholder": "​",
            "style": "IPY_MODEL_45347d008b734ea5a5a48ac6a2875992",
            "value": " 801/801 [00:00&lt;00:00, 40.4kB/s]"
          }
        },
        "c1ffd6dd4ab14a1998250a6c5e30b243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9911fde248db4c3b9aa4c00e05df2ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30700761571e4e5182aaa3fbaa9a5011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efdffc6e0366476799f85a7ba3a8d022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38f5c97555749a7a49236bf774e76b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0347f23930aa45668b6ee328a7361443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45347d008b734ea5a5a48ac6a2875992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928068331fa4435fbba48f22df63b5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_944f6d40d5324a67965004acc426c803",
              "IPY_MODEL_8d4d95f5c48b4927b8f22d3a211b5e76",
              "IPY_MODEL_b60c47d692fa4f6d95c6c07508cbb34e"
            ],
            "layout": "IPY_MODEL_e0664f2f35b3414f9fa85b818629d6f2"
          }
        },
        "944f6d40d5324a67965004acc426c803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d36885edfc3486ea8248557c82f2ece",
            "placeholder": "​",
            "style": "IPY_MODEL_26ce8282139f47daabbf78a46413b1f2",
            "value": "model.safetensors: 100%"
          }
        },
        "8d4d95f5c48b4927b8f22d3a211b5e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650509c288f0450e8ca8b489de2ff2e6",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca748f9ef5204a5ea3ab455483c951a9",
            "value": 2239618772
          }
        },
        "b60c47d692fa4f6d95c6c07508cbb34e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87bc867b38f42f89fdf4639a58f7ce6",
            "placeholder": "​",
            "style": "IPY_MODEL_dd6f010748cd4d0489a6c4fe13a16f22",
            "value": " 2.24G/2.24G [00:13&lt;00:00, 39.1MB/s]"
          }
        },
        "e0664f2f35b3414f9fa85b818629d6f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d36885edfc3486ea8248557c82f2ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ce8282139f47daabbf78a46413b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "650509c288f0450e8ca8b489de2ff2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca748f9ef5204a5ea3ab455483c951a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e87bc867b38f42f89fdf4639a58f7ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6f010748cd4d0489a6c4fe13a16f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45a44fecb0c44335880715348f217873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97e6f2cecb73485887dc96387934a9ca",
              "IPY_MODEL_bd75d111fb3b41229b53090875cf2438",
              "IPY_MODEL_8c75a641e4844e949f8518695d145fe1"
            ],
            "layout": "IPY_MODEL_00bbcfbbcfb04ffd87b2db7024bf774e"
          }
        },
        "97e6f2cecb73485887dc96387934a9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd763075690414eb72217575f8385ca",
            "placeholder": "​",
            "style": "IPY_MODEL_c106d24685cd446f83e07583d6c8922d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bd75d111fb3b41229b53090875cf2438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d455508db49c4eec827f80d2faf6f451",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c59ab837029744cbb3bec9da562d4f15",
            "value": 443
          }
        },
        "8c75a641e4844e949f8518695d145fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3810ccb0aa9c4de5a1f3a9ae0641b61a",
            "placeholder": "​",
            "style": "IPY_MODEL_68a0e96dc827429e8cab3a6ed1b3a7fc",
            "value": " 443/443 [00:00&lt;00:00, 33.5kB/s]"
          }
        },
        "00bbcfbbcfb04ffd87b2db7024bf774e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd763075690414eb72217575f8385ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c106d24685cd446f83e07583d6c8922d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d455508db49c4eec827f80d2faf6f451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59ab837029744cbb3bec9da562d4f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3810ccb0aa9c4de5a1f3a9ae0641b61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a0e96dc827429e8cab3a6ed1b3a7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "415d2153fe5846f4bb49038101342cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51f0e3c5e3da4860bc9c9e52f855de59",
              "IPY_MODEL_adde68fb7f1d4a59b61d56facb7a9e36",
              "IPY_MODEL_b6a1081bbbe04322ae95bd561b66785f"
            ],
            "layout": "IPY_MODEL_6813061815ff48aeb2bad919058b16ee"
          }
        },
        "51f0e3c5e3da4860bc9c9e52f855de59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2dba78cad64b829252c48619e2a5da",
            "placeholder": "​",
            "style": "IPY_MODEL_8f9f3ca83fde4493b1c6bfa456917a3a",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "adde68fb7f1d4a59b61d56facb7a9e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef0530371a84737a7e1c8d5df59609b",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd4acc8cb7a1492499cf15006477544b",
            "value": 5069051
          }
        },
        "b6a1081bbbe04322ae95bd561b66785f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30385d8381b4a02bd91b3f7d0e2c277",
            "placeholder": "​",
            "style": "IPY_MODEL_bc5714edea4747e193a66aed7ba590c5",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 105MB/s]"
          }
        },
        "6813061815ff48aeb2bad919058b16ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2dba78cad64b829252c48619e2a5da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f9f3ca83fde4493b1c6bfa456917a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ef0530371a84737a7e1c8d5df59609b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd4acc8cb7a1492499cf15006477544b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e30385d8381b4a02bd91b3f7d0e2c277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5714edea4747e193a66aed7ba590c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b04cf1110f544da3b6421043f1e92aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b84767cde5184af08edc17f56fbb07fd",
              "IPY_MODEL_eeadb21f1c904f529ebc901a8c3d324e",
              "IPY_MODEL_20661349f99345aeb13c42f4ce55f53a"
            ],
            "layout": "IPY_MODEL_f69cd437032f47af910fee6210dd6582"
          }
        },
        "b84767cde5184af08edc17f56fbb07fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8566295335704aa7bfe6dc1aabbb8abe",
            "placeholder": "​",
            "style": "IPY_MODEL_e726e7938cc048188be6c2d74c85484f",
            "value": "tokenizer.json: 100%"
          }
        },
        "eeadb21f1c904f529ebc901a8c3d324e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c43fcf4b710d4d149c109ef6b55a0ff8",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf8043dbb00a41dd804c19fc2b4d0001",
            "value": 17098107
          }
        },
        "20661349f99345aeb13c42f4ce55f53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd7016a6a19c4a6db5d2e2b1a726ec9e",
            "placeholder": "​",
            "style": "IPY_MODEL_70e3455b21594f89a751a227a38f8498",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 228MB/s]"
          }
        },
        "f69cd437032f47af910fee6210dd6582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8566295335704aa7bfe6dc1aabbb8abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e726e7938cc048188be6c2d74c85484f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c43fcf4b710d4d149c109ef6b55a0ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf8043dbb00a41dd804c19fc2b4d0001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd7016a6a19c4a6db5d2e2b1a726ec9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e3455b21594f89a751a227a38f8498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a73499bf912749638d37c258fdc2720e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61305f0995eb433dbbb77ccb8fb8ed24",
              "IPY_MODEL_bcb65aafe409429f8bb92347c26336f7",
              "IPY_MODEL_9d16f2b238c445f18bff6050026fe33d"
            ],
            "layout": "IPY_MODEL_2835360b19e0435e8da2ec671e3e6d9b"
          }
        },
        "61305f0995eb433dbbb77ccb8fb8ed24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea41ad9e0a640fab265fce724ee2294",
            "placeholder": "​",
            "style": "IPY_MODEL_ccf21075549346baac5db4b095c7d83f",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "bcb65aafe409429f8bb92347c26336f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6694e71ff2425ca09e1230e98f6f76",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3854f3790335469bad196cc961b9bdcb",
            "value": 279
          }
        },
        "9d16f2b238c445f18bff6050026fe33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6eb7f961c942c5a91d764124fa3e96",
            "placeholder": "​",
            "style": "IPY_MODEL_d4fcb3a89f894be0a7ba7dd688ae7218",
            "value": " 279/279 [00:00&lt;00:00, 22.5kB/s]"
          }
        },
        "2835360b19e0435e8da2ec671e3e6d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea41ad9e0a640fab265fce724ee2294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf21075549346baac5db4b095c7d83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e6694e71ff2425ca09e1230e98f6f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3854f3790335469bad196cc961b9bdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f6eb7f961c942c5a91d764124fa3e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4fcb3a89f894be0a7ba7dd688ae7218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalai2315/Langchain_for_AgentiAI/blob/main/Document_Retriever_Search_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Build a Document Retriever Search Engine on Wikipedia Data"
      ],
      "metadata": {
        "id": "TkZ-tYIsqsAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.10\n",
        "!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.11\n",
        "!pip install langchain-huggingface==0.1.2\n",
        "!pip install jq==1.8.0\n",
        "!pip install pymupdf==1.25.1"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe86509-3970-4527-ebb3-3a22d106920b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.22)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n",
            "Collecting langchain-openai==0.2.12\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.22)\n",
            "Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n",
            "  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n",
            "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed langchain-openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n",
            "Collecting langchain-community==0.3.11\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n",
            "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.22\n",
            "    Uninstalling langchain-core-0.3.22:\n",
            "      Successfully uninstalled langchain-core-0.3.22\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.10\n",
            "    Uninstalling langchain-0.3.10:\n",
            "      Successfully uninstalled langchain-0.3.10\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Collecting langchain-huggingface==0.1.2\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface==0.1.2) (0.26.5)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface==0.1.2) (0.3.24)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface==0.1.2) (3.2.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface==0.1.2) (0.20.3)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface==0.1.2) (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.1.2\n",
            "Collecting jq==1.8.0\n",
            "  Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.8.0\n",
            "Collecting pymupdf==1.25.1\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB and LangChain wrapper"
      ],
      "metadata": {
        "id": "TlfidBdQZRGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ],
      "metadata": {
        "id": "uZKQDgQURhmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ecf7f3-e9a5-4188-b82f-dbe46ea54c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.3.24)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (1.26.4)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.9.4)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.4.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=14ae5026846402425da7285ecf23de986a88053f75c39fc9e1fbaed891048fc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145f2878-adc8-49d8-e771-7ffaf8c0e594"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "M8nHAP7XOGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "jzrIVI2NAHC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Data"
      ],
      "metadata": {
        "id": "afzeN_WkHIz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n",
        "# manually upload it on colab\n",
        "!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d7d382-8f63-4408-bcc1-0b9841a3cd1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n",
            "To: /content/rag_docs.zip\n",
            "\r  0% 0.00/5.92M [00:00<?, ?B/s]\r100% 5.92M/5.92M [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rag_docs.zip"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8761fda2-8d28-434d-ee35-eba8b71dfa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rag_docs.zip\n",
            "   creating: rag_docs/\n",
            "  inflating: rag_docs/attention_paper.pdf  \n",
            "  inflating: rag_docs/cnn_paper.pdf  \n",
            "  inflating: rag_docs/resnet_paper.pdf  \n",
            "  inflating: rag_docs/vision_transformer.pdf  \n",
            "  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load JSON Documents from Wikipedia Dump"
      ],
      "metadata": {
        "id": "EGbpHRo0S_Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n",
        "                    jq_schema='.',\n",
        "                    text_content=False,\n",
        "                    json_lines=True)\n",
        "wiki_docs = loader.load()"
      ],
      "metadata": {
        "id": "RZ5y0NfzHPhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d210508-4195-4a8b-b8e5-ac3f5e0b90ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs[1500]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89cf936-0973-47cc-a692-d1b7cce0da21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 1501}, page_content='{\"id\": \"460169\", \"title\": \"Janne Persson\", \"paragraphs\": [\"Jan Persson (\\\\\"Janne Lucas\\\\\"), born 3 October 1947 in Gothenburg\\'s Gamlestad Parish in Gothenburg, Sweden is a Swedish pianist and singer, scoring several chart successes in Sweden during the 1970s and 1980s. Janne Lucas participated at Melodifestivalen 1980 with the song \\\\\"V\\\\u00e4xeln hall\\\\u00e5\\\\\", winning the contest. The upcoming year he participated with the song \\\\\"Rocky Mountain\\\\\" ending up third.\", \"For many years, Janne Lucas also acted as pianist for \\\\\"Vi i femman\\\\\"\", \"Janne also accompanied the vocal group \\\\\"Noviserna\\\\\" for a while, where Anna-Lisa Cederquist participated.\"]}')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "wiki_docs_processed = []\n",
        "\n",
        "for doc in wiki_docs:\n",
        "    doc = json.loads(doc.page_content)\n",
        "    metadata = {\n",
        "        \"title\": doc['title'],\n",
        "        \"id\": doc['id'],\n",
        "        \"source\": \"Wikipedia\"\n",
        "    }\n",
        "    data = ' '.join(doc['paragraphs'])\n",
        "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
      ],
      "metadata": {
        "id": "yICyAF85h2DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs_processed[1500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IATrHWKh7II",
        "outputId": "ee97c08c-d7aa-437b-cca2-62090b72a1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'title': 'Janne Persson', 'id': '460169', 'source': 'Wikipedia'}, page_content='Jan Persson (\"Janne Lucas\"), born 3 October 1947 in Gothenburg\\'s Gamlestad Parish in Gothenburg, Sweden is a Swedish pianist and singer, scoring several chart successes in Sweden during the 1970s and 1980s. Janne Lucas participated at Melodifestivalen 1980 with the song \"Växeln hallå\", winning the contest. The upcoming year he participated with the song \"Rocky Mountain\" ending up third. For many years, Janne Lucas also acted as pianist for \"Vi i femman\" Janne also accompanied the vocal group \"Noviserna\" for a while, where Anna-Lisa Cederquist participated.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create function to generate contextual summaries for chunks\n",
        "\n",
        "Here we borrow inspiration from Anthropic's [contextual retrieval](https://www.anthropic.com/news/contextual-retrieval) strategy which involves create a contextual summary for each chunk and adding it to the chunk before storing in the vector database.\n",
        "\n",
        "![](https://i.imgur.com/cjnB831.png)"
      ],
      "metadata": {
        "id": "h4azJnEiTGVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load PDF files with langchain\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"./rag_docs/attention_paper.pdf\")\n",
        "doc_pages = loader.load()"
      ],
      "metadata": {
        "id": "JEzkQA7yh-Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=3500,\n",
        "                                          chunk_overlap=0)\n",
        "doc_chunks = splitter.split_documents(doc_pages)"
      ],
      "metadata": {
        "id": "BNpfG-sxiikd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNRsfO-5j6GJ",
        "outputId": "ca6c24e6-f56a-4477-a83c-0fc682946400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the actual research paper\n",
        "big_doc = '\\n'.join([doc.page_content for doc in doc_chunks])"
      ],
      "metadata": {
        "id": "kAiJP-d_ikYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(big_doc.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME766Jqgk19N",
        "outputId": "aa55e738-71f3-42bf-8c1c-97ad38bd6ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "jxHHyhlbl_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chat prompt\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "\n",
        "def generate_chunk_context(document, chunk):\n",
        "\n",
        "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
        "                            Your task is to provide brief, relevant context for a chunk of text\n",
        "                            based on the following research paper.\n",
        "\n",
        "                            Here is the research paper:\n",
        "                            <paper>\n",
        "                            {paper}\n",
        "                            </paper>\n",
        "\n",
        "                            Here is the chunk we want to situate within the whole document:\n",
        "                            <chunk>\n",
        "                            {chunk}\n",
        "                            </chunk>\n",
        "\n",
        "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
        "                            considering the following guidelines:\n",
        "\n",
        "                            - Give a short succinct context to situate this chunk within the overall document\n",
        "                            for the purposes of improving search retrieval of the chunk.\n",
        "                            - Answer only with the succinct context and nothing else.\n",
        "                            - Context should be mentioned like 'Focuses on ....'\n",
        "                            do not mention 'this chunk or section focuses on...'\n",
        "\n",
        "                            Context:\n",
        "                        \"\"\"\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
        "\n",
        "    agentic_chunk_chain = (prompt_template\n",
        "                                |\n",
        "                            chatgpt\n",
        "                                |\n",
        "                            StrOutputParser())\n",
        "\n",
        "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "MHSh0Vg-mIUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_chunks[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2_HY_1AnU02",
        "outputId": "14c1ad3c-84eb-405d-cba9-c5c9169ecc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
            "where headi = Attention(QW Q\n",
            "i , KW K\n",
            "i , V W V\n",
            "i )\n",
            "Where the projections are parameter matrices W Q\n",
            "i\n",
            "∈Rdmodel×dk, W K\n",
            "i\n",
            "∈Rdmodel×dk, W V\n",
            "i\n",
            "∈Rdmodel×dv\n",
            "and W O ∈Rhdv×dmodel.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3\n",
            "Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[38, 2, 9].\n",
            "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3\n",
            "Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
            "(2)\n",
            "While the linear transformations are the same across different positions, they use different parameters\n",
            "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
            "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
            "dff = 2048.\n",
            "3.4\n",
            "Embeddings and Softmax\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
            "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
            "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
            "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
            "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_chunk_context(big_doc, doc_chunks[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "UBfwKpMpmfrn",
        "outputId": "af35a261-7902-4015-f8a2-86f282f264b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Describes the implementation of multi-head attention in the Transformer model, detailing how multiple attention heads allow the model to capture diverse information from different representation subspaces. It explains the mathematical formulation of multi-head attention and its application in both encoder-decoder and self-attention layers, emphasizing the importance of maintaining the auto-regressive property in the decoder. Additionally, it introduces the position-wise feed-forward networks that complement the attention mechanisms.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process PDF Documents"
      ],
      "metadata": {
        "id": "8NPYIw9MTt6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def create_contextual_chunks(file_path):\n",
        "\n",
        "    print('Loading pages:', file_path)\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    doc_pages = loader.load()\n",
        "\n",
        "    print('Chunking pages:', file_path)\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=3500,\n",
        "                                              chunk_overlap=0)\n",
        "    doc_chunks = splitter.split_documents(doc_pages)\n",
        "\n",
        "    print('Generating contextual chunks:', file_path)\n",
        "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
        "    contextual_chunks = []\n",
        "    for chunk in doc_chunks:\n",
        "        context = generate_chunk_context(original_doc, chunk.page_content)\n",
        "        contextual_chunks.append(Document(page_content=context+'\\n'+chunk.page_content,\n",
        "                                          metadata=chunk.metadata))\n",
        "    print('Finished processing:', file_path)\n",
        "    print()\n",
        "    return contextual_chunks"
      ],
      "metadata": {
        "id": "-uxOSfcsxqHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "pdf_files = glob('./rag_docs/*.pdf')\n",
        "pdf_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VUpdmczt0C",
        "outputId": "f0b67a7c-da7a-4d52-9538-3136b0a428d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./rag_docs/cnn_paper.pdf',\n",
              " './rag_docs/attention_paper.pdf',\n",
              " './rag_docs/resnet_paper.pdf',\n",
              " './rag_docs/vision_transformer.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs = []\n",
        "for fp in pdf_files:\n",
        "    paper_docs.extend(create_contextual_chunks(fp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsicBMPazzlg",
        "outputId": "a91291d5-ec3a-4694-c78b-f755550ab7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pages: ./rag_docs/cnn_paper.pdf\n",
            "Chunking pages: ./rag_docs/cnn_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/cnn_paper.pdf\n",
            "Finished processing: ./rag_docs/cnn_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/attention_paper.pdf\n",
            "Chunking pages: ./rag_docs/attention_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/attention_paper.pdf\n",
            "Finished processing: ./rag_docs/attention_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/resnet_paper.pdf\n",
            "Chunking pages: ./rag_docs/resnet_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/resnet_paper.pdf\n",
            "Finished processing: ./rag_docs/resnet_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/vision_transformer.pdf\n",
            "Chunking pages: ./rag_docs/vision_transformer.pdf\n",
            "Generating contextual chunks: ./rag_docs/vision_transformer.pdf\n",
            "Finished processing: ./rag_docs/vision_transformer.pdf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOkwPMxp0gFh",
        "outputId": "7ee8d809-322f-454b-8039-33d2d518f5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVK2i0lR0ieV",
        "outputId": "810656d7-ca73-4e80-ab56-060db3e38ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './rag_docs/cnn_paper.pdf', 'file_path': './rag_docs/cnn_paper.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\\nAn Introduction to Convolutional Neural Networks\\nKeiron O’Shea1 and Ryan Nash2\\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\\nkeo7@aber.ac.uk\\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\\n4YW\\nnashrd@live.lancs.ac.uk\\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\\nbiologically inspired computational models are able to far exceed the per-\\nformance of previous forms of artiﬁcial intelligence in common machine\\nlearning tasks. One of the most impressive forms of ANN architecture is\\nthat of the Convolutional Neural Network (CNN). CNNs are primarily\\nused to solve difﬁcult image-driven pattern recognition tasks and with\\ntheir precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently\\npublished papers and newly formed techniques in developing these bril-\\nliantly fantastic image recognition models. This introduction assumes you\\nare familiar with the fundamentals of ANNs and machine learning.\\nKeywords: Pattern recognition, artiﬁcial neural networks, machine learn-\\ning, image analysis\\n1\\nIntroduction\\nArtiﬁcial Neural Networks (ANNs) are computational processing systems of\\nwhich are heavily inspired by way biological nervous systems (such as the hu-\\nman brain) operate. ANNs are mainly comprised of a high number of intercon-\\nnected computational nodes (referred to as neurons), of which work entwine in\\na distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would\\nload the input, usually in the form of a multidimensional vector to the input\\nlayer of which will distribute it to the hidden layers. The hidden layers will then\\nmake decisions from the previous layer and weigh up how a stochastic change\\nwithin itself detriments or improves the ﬁnal output, and this is referred to as\\nthe process of learning. Having multiple hidden layers stacked upon each-other\\nis commonly called deep learning.\\narXiv:1511.08458v2  [cs.NE]  2 Dec 2015')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbtpR-r50mEn",
        "outputId": "553d9b79-b92f-47cc-9cad-6e3b3fc7c00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = wiki_docs_processed + paper_docs\n",
        "len(total_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQWgq9t0pMH",
        "outputId": "531355e6-6ed7-4284-d572-3b167d669b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1880"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Databases\n",
        "\n",
        "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector database takes care of storing embedded data and performing vector search for you.\n",
        "\n",
        "### Chroma Vector DB\n",
        "\n",
        "[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0."
      ],
      "metadata": {
        "id": "IO3_BzTbTBcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector DB and persist on disk\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "-PnV9lAXZw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=total_docs,\n",
        "                                  collection_name='my_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./my_db\")"
      ],
      "metadata": {
        "id": "kRYfcrsHUxyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "### Load Vector DB from disk\n",
        "\n",
        "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./my_db\",\n",
        "                   collection_name='my_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC3uPqYop0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27548dc7-8a03-4fcb-b861-473976d0b819"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7a31d82555d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with Vector Database Retrievers\n",
        "\n",
        "Here we will explore the following retrieval strategies on our Vector Database:\n",
        "\n",
        "- Similarity or Ranking based Retrieval\n",
        "- Multi Query Retrieval\n",
        "- Contextual Compression Retrieval\n",
        "- Chained Retrieval Pipeline"
      ],
      "metadata": {
        "id": "bprZC4S6TLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity or Ranking based Retrieval\n",
        "\n",
        "We use cosine similarity here and retrieve the top 5 similar documents based on the user input query"
      ],
      "metadata": {
        "id": "269fFuebBr5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "ST7PDWG2b0ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_docs(docs):\n",
        "    for doc in docs:\n",
        "        print('Metadata:', doc.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(doc.page_content[:1000]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "J3G1vN3U2QJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is machine learning?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "4czRBjWLe8JH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "fdda3ffe-802f-4a00-bd20-e9f29cf021c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '6360', 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is ML?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "E_1DQav832gr",
        "outputId": "8ef6fbb6-b563-4da2-f6ac-a3a7fb4116c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '312307', 'source': 'Wikipedia', 'title': 'Standard ML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Standard ML is a functional programming language which is a dialect of ML (programming language). It is sometimes used for writing compilers and in theorem provers. Here is an example of a factorial function written in a simple, non-tail recursive, style."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '15798', 'source': 'Wikipedia', 'title': 'Major League Baseball'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Major League Baseball (MLB) is a professional baseball league in North America. It is often considered to be the highest level of professional baseball in the world. There are two leagues that make up the MLB: the American League, also called AL, and National League, also called NL. There are currently 30 teams in the MLB, 29 from the United States and one from Canada, the Toronto Blue Jays. The official website of MLB is known as \"MLB.com\" (www.mlb.com). The 30 teams in MLB are divided into two leagues: American and National. Each league is divided into three divisions: East, Central, West. Since the 2013 season, each division has had five teams. The most recent change took place after the 2012 season, when the Houston Astros moved from the NL Central to the AL West."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '196959', 'source': 'Wikipedia', 'title': 'Mathematical Reviews'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mathematical Reviews is a journal and online database published by the American Mathematical Society that contains many articles in mathematics, statistics, and related topics."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '757418', 'source': 'Wikipedia', 'title': 'VRML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "VRML (Virtual Reality Modeling Language, pronounced \"vermal\", or by its initials, known before 1995 as Virtual Reality Markup Language) is a standard 3-dimensional (3D) interactive vector graphics file format designed for the World Wide Web. It has been succeeded by X3D. VRML uses text files. The vertices, edges, surface colors, UV-mapped textures, shininess, transparency and more of a 3D polygon can be specified. Graphical components can be made to fetch web pages or other VRML files from the Internet from URLs when the user clicks on the graphical component. Animations, sounds, lighting, and other things about the virtual world can interact with the user or can happen when external events say so, such as timers. A special Script Node allows program code (such as program code in Java or ECMAScript) to be added to a VRML file. VRML files are commonly called \"worlds\" and have the .wrl extension (for example, a VRML file can be called island.wrl). VRML files are in plain text and usually"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the difference between transformers and vision transformers?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "CYGmZdV--qz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6323ba0-e452-4c78-9f83-fe007620e103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 7, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents evidence that a pure Transformer can achieve competitive performance on various image recognition benchmarks when pre-trained on large datasets.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 2, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁ"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 7, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications of localized attention in hybrid models that incorporate convolutional networks. It also discusses the relationship between attention distance and network depth, emphasizing the model's ability to attend to semantically relevant image regions for classification tasks.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 1, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Vision Transformer (ViT) in comparison to convolutional neural networks (CNNs), highlighting how large-scale training on extensive datasets enhances its generalization capabilities. It discusses the results achieved by ViT when pre-trained on datasets like ImageNet-21k and JFT-300M, demonstrating its competitive accuracy on various image recognition benchmarks. Additionally, it references related work that explores the integration of self-attention mechanisms in image processing.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. W"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a cnn?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "ZJmJRjlV_kvS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fcb684a-14e6-4d9d-bd00-753215a67826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '3615', 'source': 'Wikipedia', 'title': 'CNN'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Cable News Network (CNN) is an American cable news television channel. It was founded in 1980 by Ted Turner. The Cable News Network first aired on television on June 1, 1980. The Cable News Network's first newscast was anchored (hosted) by David Walker and his wife Lois Hart. In its first year CNN hired many political analysts, including Rowland Evans and Robert Novak. On January 1, 1982 CNN launched a 24-hour sister newscast channel with no talk shows or commentary shows called CNN2. CNN broadcasts programs from its headquarters at the CNN Center in Atlanta, or from the Time Warner Center in New York City, or from studios in Washington, D.C., and Los Angeles. CNN is owned by Time Warner, and the U.S. news channel is a part of the Turner Broadcasting System. The hosts of its opinion shows are Don Lemon, Chris Cuomo, Fredricka Whitfield, Erin Burnett, Brianna Keiler and Brooke Baldwin. CNN has been criticized by the right-wing Media Research Center for having a left-wing bias. Accor"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architectural design of Convolutional Neural Networks (CNNs), specifically the practice of stacking multiple convolutional layers before pooling layers to enhance feature extraction. It discusses the benefits of using smaller convolutional layers to manage computational complexity and memory allocation, while also addressing the importance of input dimensionality and zero-padding in CNN configurations.\nIntroduction to Convolutional Neural Networks\n9\nAnother common CNN architecture is to stack two convolutional layers before\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\nstacking multiple convolutional layers allows for more complex features of the\ninput vector to be selected.\ninput\nconvolution w/ ReLu\npooling\nconvolution\nw/ ReLu\npooling\nfully-connected\nw/ ReLu\nfully-connected\nconvolution w/ ReLu\npooling\n0\n9\noutput \n...\nFig. 5: A common form of CNN architecture in which convolutional layers are\nstacked between ReLus continuously before bei"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 3, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the structural differences between Convolutional Neural Networks (CNNs) and traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons in CNNs. It outlines the significance of spatial dimensions and depth in the input volume, and introduces the three main types of layers that constitute a CNN architecture, including convolutional, pooling, and fully-connected layers.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the foundational concepts of artificial neural networks (ANNs), including their structure and learning paradigms, specifically supervised and unsupervised learning. It highlights the similarities between convolutional neural networks (CNNs) and traditional ANNs, emphasizing the unique application of CNNs in image pattern recognition and the encoding of image-specific features within their architecture.\n2\nKeiron O’Shea et al.\nInput 1\nInput 2\nInput 3\nInput 4\nInput Layer\nHidden Layer\nOutput Layer\nOutput\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\nof a input layer, a hidden layer and an output layer. This structure is the basis\nof a number of common ANN architectures, included but not limited to Feed-\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\nRecurrent Neural Networks (RNNs).\nThe two key learning paradigms in image processing tasks are supervised and\nunsupervised learning. Supervised learning is learning through pre-la"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is deep learning?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "9lfhHB7gQP3Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "outputId": "acc9a415-0287-4886-e469-0260c8feb06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151211011345Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/resnet_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151211011345Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/resnet_paper.pdf', 'subject': '', 'title': '', 'total_pages': 12, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of deep residual learning as a framework to facilitate the training of significantly deeper neural networks, addressing challenges such as vanishing gradients and degradation of accuracy. It highlights the empirical success of residual networks on the ImageNet dataset and their application in various visual recognition tasks, including object detection and segmentation, leading to top performances in major competitions.\nDeep Residual Learning for Image Recognition\nKaiming He\nXiangyu Zhang\nShaoqing Ren\nJian Sun\nMicrosoft Research\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\nAbstract\nDeeper neural networks are more difﬁcult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidenc"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151211011345Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/resnet_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151211011345Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/resnet_paper.pdf', 'subject': '', 'title': '', 'total_pages': 12, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the deep residual learning framework to address the degradation problem in training deep neural networks. It explains the concept of residual mapping, the use of shortcut connections, and presents empirical evidence demonstrating the effectiveness of residual networks in achieving higher accuracy and easier optimization compared to traditional plain networks. Additionally, it highlights the success of these methods in various image recognition tasks, including the ImageNet competition.\nidentity\nweight layer\nweight layer\nrelu\nrelu\nF(x)\u0001+\u0001x\nx\nF(x)\nx\nFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework.\nIn-\nstead of hoping each few stacked layers directly ﬁt a\ndesired underlying mapping, we explicitly let these lay-\ners ﬁt a residual mapping. Formally, denoting the des"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151211011345Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/resnet_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151211011345Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/resnet_paper.pdf', 'subject': '', 'title': '', 'total_pages': 12, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the challenges associated with training deeper neural networks, specifically addressing the degradation problem where increasing the number of layers leads to higher training errors. It discusses the theoretical expectation that deeper models should not perform worse than shallower ones if constructed correctly, yet empirical evidence shows that current optimization methods struggle to achieve this.\nmore layers to a suitably deep model leads to higher train-\ning error, as reported in [11, 42] and thoroughly veriﬁed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indica"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151211011345Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/resnet_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151211011345Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/resnet_paper.pdf', 'subject': '', 'title': '', 'total_pages': 12, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Provides a list of references that support various concepts and methodologies discussed throughout the paper, including advancements in deep learning techniques, object detection frameworks, and the performance of convolutional neural networks. These references are integral to understanding the theoretical foundations and empirical results presented in the context of deep residual learning and its applications in image recognition tasks.\n[28] G. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In NIPS, 2014.\n[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\nboltzmann machines. In ICML, 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR, 2007.\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS, 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object det"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is nlp?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "cT3CFWhVRo2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "27baf69b-7e35-4e10-c0ac-df611408f667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '335464', 'source': 'Wikipedia', 'title': 'Neurolinguistic programming'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Neurolinguistic programming is a way of communicating, created in the 1970s. It is often shortened to \"NLP\". The discipline assumes there is a link between neurological processes, language and behavior. According to NLP, it is possible to achieve certain goals in life by changing one's behaviour. Certain neuroscientists psychologists and linguists, believe that NLP is unsupported by current scientific evidence and that it uses incorrect and misleading terms and concepts. NLP was invented by Richard Bandler and John Grinder. According to these people, NLP can help solve problems such as phobias, depression, habit disorder, psychosomatic illnesses, and learning disorders."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '40613', 'source': 'Wikipedia', 'title': 'Natural language processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field in Artificial Intelligence, and is also related to linguistics. On a high level, the goal of NLP is to program computers to automatically understand human languages, and also to automatically write/speak in human languages. We say \"Natural Language\" to mean human language, and to indicate that we are not talking about computer (programming) languages."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '669662', 'source': 'Wikipedia', 'title': 'Loop AI Labs'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University's . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK's business clients, which cover 2"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Query Retrieval\n",
        "\n",
        "Retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
        "\n",
        "The [`MultiQueryRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents."
      ],
      "metadata": {
        "id": "6HKdIe2_wIBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "qmfVRrRujxrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "mq_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever, llm=chatgpt\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "# so we can see what queries are generated by the LLM\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "s3gU4HPvfjs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a cnn?\"\n",
        "top_docs = mq_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "KZJBIsl8xC_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39db8225-55c1-4fb1-f360-432358f33249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does CNN stand for and what are its main functions?  ', 'Can you explain the concept and applications of a convolutional neural network?  ', 'What are the key features and uses of CNNs in machine learning?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '3615', 'source': 'Wikipedia', 'title': 'CNN'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Cable News Network (CNN) is an American cable news television channel. It was founded in 1980 by Ted Turner. The Cable News Network first aired on television on June 1, 1980. The Cable News Network's first newscast was anchored (hosted) by David Walker and his wife Lois Hart. In its first year CNN hired many political analysts, including Rowland Evans and Robert Novak. On January 1, 1982 CNN launched a 24-hour sister newscast channel with no talk shows or commentary shows called CNN2. CNN broadcasts programs from its headquarters at the CNN Center in Atlanta, or from the Time Warner Center in New York City, or from studios in Washington, D.C., and Los Angeles. CNN is owned by Time Warner, and the U.S. news channel is a part of the Turner Broadcasting System. The hosts of its opinion shows are Don Lemon, Chris Cuomo, Fredricka Whitfield, Erin Burnett, Brianna Keiler and Brooke Baldwin. CNN has been criticized by the right-wing Media Research Center for having a left-wing bias. Accor"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 3, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the structural differences between Convolutional Neural Networks (CNNs) and traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons in CNNs. It outlines the significance of spatial dimensions and depth in the input volume, and introduces the three main types of layers that constitute a CNN architecture, including convolutional, pooling, and fully-connected layers.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architectural design of Convolutional Neural Networks (CNNs), specifically the practice of stacking multiple convolutional layers before pooling layers to enhance feature extraction. It discusses the benefits of using smaller convolutional layers to manage computational complexity and memory allocation, while also addressing the importance of input dimensionality and zero-padding in CNN configurations.\nIntroduction to Convolutional Neural Networks\n9\nAnother common CNN architecture is to stack two convolutional layers before\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\nstacking multiple convolutional layers allows for more complex features of the\ninput vector to be selected.\ninput\nconvolution w/ ReLu\npooling\nconvolution\nw/ ReLu\npooling\nfully-connected\nw/ ReLu\nfully-connected\nconvolution w/ ReLu\npooling\n0\n9\noutput \n...\nFig. 5: A common form of CNN architecture in which convolutional layers are\nstacked between ReLus continuously before bei"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 4, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and functionality of Convolutional Neural Networks (CNNs), detailing the roles of activation functions, pooling layers, and fully-connected layers in transforming input data into class scores for classification and regression tasks. It emphasizes the importance of understanding individual layers and their hyperparameters for effective model creation and optimization.\nIntroduction to Convolutional Neural Networks\n5\nan ’elementwise’ activation function such as sigmoid to the output of the\nactivation produced by the previous layer.\n3. The pooling layer will then simply perform downsampling along the spa-\ntial dimensionality of the given input, further reducing the number of pa-\nrameters within that activation.\n4. The fully-connected layers will then perform the same duties found in\nstandard ANNs and attempt to produce class scores from the activations,\nto be used for classiﬁcation. It is also suggested that ReLu may be used\nbetween these layers, as to improve p"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 5, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the function and significance of kernels in convolutional layers of Convolutional Neural Networks (CNNs), detailing how these kernels operate on input data to produce activation maps. It explains the concept of receptive fields and the reduction of model complexity through hyperparameters such as depth, stride, and zero-padding.\n6\nKeiron O’Shea et al.\nThese kernels are usually small in spatial dimensionality, but spreads along the\nentirety of the depth of the input. When the data hits a convolutional layer,\nthe layer convolves each ﬁlter across the spatial dimensionality of the input to\nproduce a 2D activation map. These activation maps can be visualised, as seen\nin Figure 3.\nAs we glide through the input, the scalar product is calculated for each value in\nthat kernel. (Figure 4) From this the network will learn kernels that ’ﬁre’ when\nthey see a speciﬁc feature at a given spatial position of the input. These are\ncommonly known as activations.\n0\n0\n0\n1\n0\n2\n0\n1\n1\n4\n0\n0\n0\n0\n0\n0"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 10, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, highlighting significant contributions to the field of convolutional neural networks (CNNs) and their applications in image processing, character recognition, and object detection. These references provide foundational knowledge and advancements that support the development and understanding of CNN architectures discussed throughout the document.\nIntroduction to Convolutional Neural Networks\n11\n4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural\nnetwork committees for handwritten character classiﬁcation. In: Document Analysis\nand Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE\n(2011)\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-\nworksa review. Pattern recognition 35(10), 2279–2301 (2002)\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware\naccelerated convolutional neural networks for synthetic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the foundational concepts of artificial neural networks (ANNs), including their structure and learning paradigms, specifically supervised and unsupervised learning. It highlights the similarities between convolutional neural networks (CNNs) and traditional ANNs, emphasizing the unique application of CNNs in image pattern recognition and the encoding of image-specific features within their architecture.\n2\nKeiron O’Shea et al.\nInput 1\nInput 2\nInput 3\nInput 4\nInput Layer\nHidden Layer\nOutput Layer\nOutput\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\nof a input layer, a hidden layer and an output layer. This structure is the basis\nof a number of common ANN architectures, included but not limited to Feed-\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\nRecurrent Neural Networks (RNNs).\nThe two key learning paradigms in image processing tasks are supervised and\nunsupervised learning. Supervised learning is learning through pre-la"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 2, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the limitations of traditional artificial neural networks (ANNs) in handling image data, particularly regarding computational complexity and the risk of overfitting. It discusses the implications of increasing input dimensionality on the number of weights and the overall architecture of neural networks, emphasizing the need for reduced complexity to enhance predictive performance. Additionally, it introduces the architecture of convolutional neural networks (CNNs) as a solution tailored for image processing tasks.\nIntroduction to Convolutional Neural Networks\n3\nmore suited for image-focused tasks - whilst further reducing the parameters\nrequired to set up the model.\nOne of the largest limitations of traditional forms of ANN is that they tend to\nstruggle with the computational complexity required to compute image data.\nCommon machine learning benchmarking datasets such as the MNIST database\nof handwritten digits are suitable for most forms of ANN, due to its relatively\nsmall "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is nlp?\"\n",
        "top_docs = mq_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "TVBeusuSyVo3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4aac3f27-6ee9-4117-cb1b-5fe1fa45d7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does NLP stand for and what are its main applications?  ', 'Can you explain the concept of natural language processing and its significance?  ', 'What are the key techniques and technologies used in NLP?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '40613', 'source': 'Wikipedia', 'title': 'Natural language processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field in Artificial Intelligence, and is also related to linguistics. On a high level, the goal of NLP is to program computers to automatically understand human languages, and also to automatically write/speak in human languages. We say \"Natural Language\" to mean human language, and to indicate that we are not talking about computer (programming) languages."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '335464', 'source': 'Wikipedia', 'title': 'Neurolinguistic programming'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Neurolinguistic programming is a way of communicating, created in the 1970s. It is often shortened to \"NLP\". The discipline assumes there is a link between neurological processes, language and behavior. According to NLP, it is possible to achieve certain goals in life by changing one's behaviour. Certain neuroscientists psychologists and linguists, believe that NLP is unsupported by current scientific evidence and that it uses incorrect and misleading terms and concepts. NLP was invented by Richard Bandler and John Grinder. According to these people, NLP can help solve problems such as phobias, depression, habit disorder, psychosomatic illnesses, and learning disorders."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '669662', 'source': 'Wikipedia', 'title': 'Loop AI Labs'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University's . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK's business clients, which cover 2"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 11, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, which include foundational works in computational linguistics, parsing, attention mechanisms, and neural machine translation. These references support the development and evaluation of the Transformer model, highlighting its contributions to various natural language processing tasks.\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsumm"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '52820', 'source': 'Wikipedia', 'title': 'Signal processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Signal processing is the analysis, interpretation and manipulation of signals. Signals of interest include sound, images, biological signals such as ECG, radar signals, and many others. Processing of such signals includes storage and reconstruction, separation of information from noise (e.g., aircraft identification by radar), compression (e.g., image compression), and feature extraction (e.g., converting text to speech). For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunication. For digital signals, signal processing may involve the compression, error checking and error detection of digital signals."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the limitations of recurrent neural networks and the advantages of attention mechanisms in sequence modeling and transduction tasks. It introduces the Transformer architecture, which eliminates recurrence and leverages attention to enhance parallelization and improve translation quality. The discussion highlights the challenges of sequential computation and the need for more efficient models in natural language processing.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 10, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, which include foundational works on recurrent neural networks, attention mechanisms, and various neural architectures relevant to sequence transduction and machine translation. These references support the development and evaluation of the Transformer model, highlighting its innovations and comparisons to existing methodologies in the field.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smit"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is ML?\"\n",
        "top_docs = mq_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "G5yTCexbSUTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed24f208-1adf-438a-f73c-43696ced93e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does machine learning (ML) refer to?  ', 'Can you explain the concept of machine learning?  ', 'What are the key principles and applications of ML?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '6360', 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '312307', 'source': 'Wikipedia', 'title': 'Standard ML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Standard ML is a functional programming language which is a dialect of ML (programming language). It is sometimes used for writing compilers and in theorem provers. Here is an example of a factorial function written in a simple, non-tail recursive, style."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151211011345Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/resnet_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151211011345Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/resnet_paper.pdf', 'subject': '', 'title': '', 'total_pages': 12, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Provides a list of references that support various concepts and methodologies discussed throughout the paper, including advancements in deep learning techniques, object detection frameworks, and the performance of convolutional neural networks. These references are integral to understanding the theoretical foundations and empirical results presented in the context of deep residual learning and its applications in image recognition tasks.\n[28] G. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In NIPS, 2014.\n[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\nboltzmann machines. In ICML, 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR, 2007.\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS, 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object det"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 10, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, which include foundational works in deep learning, optimization methods, and previous studies related to image classification and representation learning. These references support the development and validation of the Vision Transformer (ViT) model presented in the paper.\nPublished as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural ne"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contextual Compression Retrieval\n",
        "\n",
        "The information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned.\n",
        "\n",
        "This compression can happen in the form of:\n",
        "\n",
        "- Remove parts of the content of retrieved documents which are not relevant to the query. This is done by extracting only relevant parts of the document to the given query\n",
        "\n",
        "- Filter out documents which are not relevant to the given query but do not remove content from the document"
      ],
      "metadata": {
        "id": "qKkMcQpf3lBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we wrap our multi-query retriever with a `ContextualCompressionRetriever`. Then we'll add an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
      ],
      "metadata": {
        "id": "pNMpbsi64QcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "\n",
        "# extracts from each document only the content that is relevant to the query\n",
        "compressor = LLMChainExtractor.from_llm(llm=chatgpt)\n",
        "\n",
        "# retrieves the documents similar to query and then applies the compressor\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=mq_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "b14n_2AHykTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is ML?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "M8yvjFWCTew6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "9bdd4311-4bd7-46d0-fd4f-5349a46781bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does machine learning (ML) refer to, and how does it work?  ', 'Can you explain the concept of machine learning and its applications?  ', 'What are the key principles and techniques involved in machine learning?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '312307', 'source': 'Wikipedia', 'title': 'Standard ML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Standard ML is a functional programming language which is a dialect of ML (programming language)."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"Neural networks are an example of machine learning, where a program can change as it learns to solve a problem.\""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is nlp?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "zW64q3FF0M-b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "48d02c91-d304-4e45-b569-a1aef75a1ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does NLP stand for and what are its main applications?  ', 'Can you explain the concept of natural language processing and its significance?  ', 'What are the key techniques and technologies involved in NLP?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '40613', 'source': 'Wikipedia', 'title': 'Natural language processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field in Artificial Intelligence, and is also related to linguistics. On a high level, the goal of NLP is to program computers to automatically understand human languages, and also to automatically write/speak in human languages. We say \"Natural Language\" to mean human language, and to indicate that we are not talking about computer (programming) languages."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '335464', 'source': 'Wikipedia', 'title': 'Neurolinguistic programming'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Neurolinguistic programming is a way of communicating, created in the 1970s. It is often shortened to \"NLP\". The discipline assumes there is a link between neurological processes, language and behavior. According to NLP, it is possible to achieve certain goals in life by changing one's behaviour. NLP was invented by Richard Bandler and John Grinder."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a cnn?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5W4wmchG6pOY",
        "outputId": "cdf94831-3f79-4c67-ec60-09d0624bc285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does CNN stand for and what are its main functions?  ', 'Can you explain the concept and applications of a convolutional neural network?  ', 'What are the key features and uses of CNNs in machine learning?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '3615', 'source': 'Wikipedia', 'title': 'CNN'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Cable News Network (CNN) is an American cable news television channel. It was founded in 1980 by Ted Turner. The Cable News Network first aired on television on June 1, 1980. CNN broadcasts programs from its headquarters at the CNN Center in Atlanta, or from the Time Warner Center in New York City, or from studios in Washington, D.C., and Los Angeles. CNN is owned by Time Warner, and the U.S. news channel is a part of the Turner Broadcasting System."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 3, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the structural differences between Convolutional Neural Networks (CNNs) and traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons in CNNs. It outlines the significance of spatial dimensions and depth in the input volume, and introduces the three main types of layers that constitute a CNN architecture, including convolutional, pooling, and fully-connected layers.\n\nOne of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input (height and the width) and the depth. The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\n\nOverall architecture\nCNNs are comprised of three types of layers. These are convolutional layers, pooling layers and f"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architectural design of Convolutional Neural Networks (CNNs), specifically the practice of stacking multiple convolutional layers before pooling layers to enhance feature extraction. It discusses the benefits of using smaller convolutional layers to manage computational complexity and memory allocation, while also addressing the importance of input dimensionality and zero-padding in CNN configurations. Introduction to Convolutional Neural Networks CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 4, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and functionality of Convolutional Neural Networks (CNNs), detailing the roles of activation functions, pooling layers, and fully-connected layers in transforming input data into class scores for classification and regression tasks. It emphasizes the importance of understanding individual layers and their hyperparameters for effective model creation and optimization. \n\nThrough this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes. \n\nAs the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 5, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the function and significance of kernels in convolutional layers of Convolutional Neural Networks (CNNs), detailing how these kernels operate on input data to produce activation maps. It explains the concept of receptive fields and the reduction of model complexity through hyperparameters such as depth, stride, and zero-padding. Convolutional layers are also able to signiﬁcantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth, the stride and setting zero-padding."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 10, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, highlighting significant contributions to the field of convolutional neural networks (CNNs) and their applications in image processing, character recognition, and object detection. These references provide foundational knowledge and advancements that support the development and understanding of CNN architectures discussed throughout the document. \n\nIntroduction to Convolutional Neural Networks"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the ﬁeld of pattern recognition within images. This allows us to encode image-speciﬁc features into the architecture, making the network."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 2, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the speciﬁc type of data. Additionally, it introduces the architecture of convolutional neural networks (CNNs) as a solution tailored for image processing tasks."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is clustering?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "onRoGL4TTk9C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "e904660c-71ac-4f78-80a4-3e3c122527c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key concepts and techniques involved in clustering?  ', 'Can you explain the different types of clustering methods and their applications?  ', 'How does clustering work, and what are its main purposes in data analysis?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '593732', 'source': 'Wikipedia', 'title': 'Cluster analysis'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a neural network?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "9QR_QNS8Scvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "233f50fb-25f5-4c27-a401-9a9fbb114658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key components and functions of a neural network?  ', 'Can you explain the concept and purpose of neural networks in machine learning?  ', 'How do neural networks work and what are their applications?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the foundational concepts of artificial neural networks (ANNs), including their structure and learning paradigms, specifically supervised and unsupervised learning. It highlights the similarities between convolutional neural networks (CNNs) and traditional ANNs, emphasizing the unique application of CNNs in image pattern recognition and the encoding of image-specific features within their architecture.\n\nA simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feed-forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\n\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a no"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artiﬁcial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the hu- man brain) operate. ANNs are mainly comprised of a high number of intercon- nected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its ﬁnal output. The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the ﬁnal output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
      ],
      "metadata": {
        "id": "URTONl5c4aLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "\n",
        "# retrieves the documents similar to query and then applies the filter\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=mq_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "fs-INms_0VTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is ML?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "bgKB3_lV17Hh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "outputId": "0de35ddf-e673-4da2-d78f-6808ea55c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does machine learning (ML) refer to?  ', 'Can you explain the concept of machine learning?  ', 'What are the key principles and applications of ML?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '312307', 'source': 'Wikipedia', 'title': 'Standard ML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Standard ML is a functional programming language which is a dialect of ML (programming language). It is sometimes used for writing compilers and in theorem provers. Here is an example of a factorial function written in a simple, non-tail recursive, style."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is NLP?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "AyhPhAld1-H3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "a108bbcc-51cb-4dda-bbaf-9a3a0dc27445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does NLP stand for and what are its main applications?  ', 'Can you explain the concept of Natural Language Processing and its significance?  ', 'What are the key techniques and technologies used in NLP?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '40613', 'source': 'Wikipedia', 'title': 'Natural language processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field in Artificial Intelligence, and is also related to linguistics. On a high level, the goal of NLP is to program computers to automatically understand human languages, and also to automatically write/speak in human languages. We say \"Natural Language\" to mean human language, and to indicate that we are not talking about computer (programming) languages."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 11, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, which include foundational works in computational linguistics, parsing, attention mechanisms, and neural machine translation. These references support the development and evaluation of the Transformer model, highlighting its contributions to various natural language processing tasks.\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsumm"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the limitations of recurrent neural networks and the advantages of attention mechanisms in sequence modeling and transduction tasks. It introduces the Transformer architecture, which eliminates recurrence and leverages attention to enhance parallelization and improve translation quality. The discussion highlights the challenges of sequential computation and the need for more efficient models in natural language processing.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a neural network?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "Ap7ddchJT0BH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "outputId": "8a2dfac6-1c1a-45f8-ba8e-bd93c1e4fd13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key components and functions of a neural network?  ', 'Can you explain the concept and workings of neural networks in simple terms?  ', 'What are the different types of neural networks and their applications?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the foundational concepts of artificial neural networks (ANNs), including their structure and learning paradigms, specifically supervised and unsupervised learning. It highlights the similarities between convolutional neural networks (CNNs) and traditional ANNs, emphasizing the unique application of CNNs in image pattern recognition and the encoding of image-specific features within their architecture.\n2\nKeiron O’Shea et al.\nInput 1\nInput 2\nInput 3\nInput 4\nInput Layer\nHidden Layer\nOutput Layer\nOutput\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\nof a input layer, a hidden layer and an output layer. This structure is the basis\nof a number of common ANN architectures, included but not limited to Feed-\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\nRecurrent Neural Networks (RNNs).\nThe two key learning paradigms in image processing tasks are supervised and\nunsupervised learning. Supervised learning is learning through pre-la"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 3, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the structural differences between Convolutional Neural Networks (CNNs) and traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons in CNNs. It outlines the significance of spatial dimensions and depth in the input volume, and introduces the three main types of layers that constitute a CNN architecture, including convolutional, pooling, and fully-connected layers.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '779314', 'source': 'Wikipedia', 'title': 'Binary Neural Network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Binary neural network is an artificial neural network, where commonly used floating-point weights are replaced with binary ones. It largely saving the storage and computation, serves as a technique for deploying deep models on resource-limited devices. Usage of binary values can bring up to 58 times speedup, while accuracy and information capacity of binary neural network can be manually controlled. Binary neural networks do not achieve the same accuracy as their full-precision counterparts, but improvements are being made to close this gap."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a cnn?\"\n",
        "top_docs = compression_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "h-0x-01-Uf2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35fcad15-b4b3-4f89-aebb-f76f41a99032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does CNN stand for and what are its main functions?  ', 'Can you explain the concept and applications of a convolutional neural network?  ', 'What are the key features and uses of CNNs in machine learning?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '3615', 'source': 'Wikipedia', 'title': 'CNN'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Cable News Network (CNN) is an American cable news television channel. It was founded in 1980 by Ted Turner. The Cable News Network first aired on television on June 1, 1980. The Cable News Network's first newscast was anchored (hosted) by David Walker and his wife Lois Hart. In its first year CNN hired many political analysts, including Rowland Evans and Robert Novak. On January 1, 1982 CNN launched a 24-hour sister newscast channel with no talk shows or commentary shows called CNN2. CNN broadcasts programs from its headquarters at the CNN Center in Atlanta, or from the Time Warner Center in New York City, or from studios in Washington, D.C., and Los Angeles. CNN is owned by Time Warner, and the U.S. news channel is a part of the Turner Broadcasting System. The hosts of its opinion shows are Don Lemon, Chris Cuomo, Fredricka Whitfield, Erin Burnett, Brianna Keiler and Brooke Baldwin. CNN has been criticized by the right-wing Media Research Center for having a left-wing bias. Accor"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 3, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the structural differences between Convolutional Neural Networks (CNNs) and traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons in CNNs. It outlines the significance of spatial dimensions and depth in the input volume, and introduces the three main types of layers that constitute a CNN architecture, including convolutional, pooling, and fully-connected layers.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 8, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architectural design of Convolutional Neural Networks (CNNs), specifically the practice of stacking multiple convolutional layers before pooling layers to enhance feature extraction. It discusses the benefits of using smaller convolutional layers to manage computational complexity and memory allocation, while also addressing the importance of input dimensionality and zero-padding in CNN configurations.\nIntroduction to Convolutional Neural Networks\n9\nAnother common CNN architecture is to stack two convolutional layers before\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\nstacking multiple convolutional layers allows for more complex features of the\ninput vector to be selected.\ninput\nconvolution w/ ReLu\npooling\nconvolution\nw/ ReLu\npooling\nfully-connected\nw/ ReLu\nfully-connected\nconvolution w/ ReLu\npooling\n0\n9\noutput \n...\nFig. 5: A common form of CNN architecture in which convolutional layers are\nstacked between ReLus continuously before bei"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 4, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and functionality of Convolutional Neural Networks (CNNs), detailing the roles of activation functions, pooling layers, and fully-connected layers in transforming input data into class scores for classification and regression tasks. It emphasizes the importance of understanding individual layers and their hyperparameters for effective model creation and optimization.\nIntroduction to Convolutional Neural Networks\n5\nan ’elementwise’ activation function such as sigmoid to the output of the\nactivation produced by the previous layer.\n3. The pooling layer will then simply perform downsampling along the spa-\ntial dimensionality of the given input, further reducing the number of pa-\nrameters within that activation.\n4. The fully-connected layers will then perform the same duties found in\nstandard ANNs and attempt to produce class scores from the activations,\nto be used for classiﬁcation. It is also suggested that ReLu may be used\nbetween these layers, as to improve p"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 5, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the function and significance of kernels in convolutional layers of Convolutional Neural Networks (CNNs), detailing how these kernels operate on input data to produce activation maps. It explains the concept of receptive fields and the reduction of model complexity through hyperparameters such as depth, stride, and zero-padding.\n6\nKeiron O’Shea et al.\nThese kernels are usually small in spatial dimensionality, but spreads along the\nentirety of the depth of the input. When the data hits a convolutional layer,\nthe layer convolves each ﬁlter across the spatial dimensionality of the input to\nproduce a 2D activation map. These activation maps can be visualised, as seen\nin Figure 3.\nAs we glide through the input, the scalar product is calculated for each value in\nthat kernel. (Figure 4) From this the network will learn kernels that ’ﬁre’ when\nthey see a speciﬁc feature at a given spatial position of the input. These are\ncommonly known as activations.\n0\n0\n0\n1\n0\n2\n0\n1\n1\n4\n0\n0\n0\n0\n0\n0"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 10, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the references cited in the research paper, highlighting significant contributions to the field of convolutional neural networks (CNNs) and their applications in image processing, character recognition, and object detection. These references provide foundational knowledge and advancements that support the development and understanding of CNN architectures discussed throughout the document.\nIntroduction to Convolutional Neural Networks\n11\n4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural\nnetwork committees for handwritten character classiﬁcation. In: Document Analysis\nand Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE\n(2011)\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-\nworksa review. Pattern recognition 35(10), 2279–2301 (2002)\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware\naccelerated convolutional neural networks for synthetic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 1, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the foundational concepts of artificial neural networks (ANNs), including their structure and learning paradigms, specifically supervised and unsupervised learning. It highlights the similarities between convolutional neural networks (CNNs) and traditional ANNs, emphasizing the unique application of CNNs in image pattern recognition and the encoding of image-specific features within their architecture.\n2\nKeiron O’Shea et al.\nInput 1\nInput 2\nInput 3\nInput 4\nInput Layer\nHidden Layer\nOutput Layer\nOutput\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\nof a input layer, a hidden layer and an output layer. This structure is the basis\nof a number of common ANN architectures, included but not limited to Feed-\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\nRecurrent Neural Networks (RNNs).\nThe two key learning paradigms in image processing tasks are supervised and\nunsupervised learning. Supervised learning is learning through pre-la"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 2, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the limitations of traditional artificial neural networks (ANNs) in handling image data, particularly regarding computational complexity and the risk of overfitting. It discusses the implications of increasing input dimensionality on the number of weights and the overall architecture of neural networks, emphasizing the need for reduced complexity to enhance predictive performance. Additionally, it introduces the architecture of convolutional neural networks (CNNs) as a solution tailored for image processing tasks.\nIntroduction to Convolutional Neural Networks\n3\nmore suited for image-focused tasks - whilst further reducing the parameters\nrequired to set up the model.\nOne of the largest limitations of traditional forms of ANN is that they tend to\nstruggle with the computational complexity required to compute image data.\nCommon machine learning benchmarking datasets such as the MNIST database\nof handwritten digits are suitable for most forms of ANN, due to its relatively\nsmall "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chained Retrieval Pipeline\n",
        "\n",
        "This strategy uses a chain of multiple retrievers sequentially to get to the most relevant documents. The following is the flow\n",
        "\n",
        "Similarity Retrieval → Compression Filter → Reranker Model Retrieval\n",
        "\n",
        "![](http://i.imgur.com/77pXxLu.gif)"
      ],
      "metadata": {
        "id": "9YK8BBMBAU9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=similarity_retriever\n",
        ")\n",
        "\n",
        "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
        "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "UgZ4eMFc2GgI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "0c2c22feca8545769f9b4b99addf7933",
            "25cda6ec52444337b87122acea111ce9",
            "d50f59269bd14d438dcba220ce51f389",
            "3413a0d7a6854d84b61b8e6e6e91c06d",
            "c1ffd6dd4ab14a1998250a6c5e30b243",
            "9911fde248db4c3b9aa4c00e05df2ea0",
            "30700761571e4e5182aaa3fbaa9a5011",
            "efdffc6e0366476799f85a7ba3a8d022",
            "b38f5c97555749a7a49236bf774e76b5",
            "0347f23930aa45668b6ee328a7361443",
            "45347d008b734ea5a5a48ac6a2875992",
            "928068331fa4435fbba48f22df63b5af",
            "944f6d40d5324a67965004acc426c803",
            "8d4d95f5c48b4927b8f22d3a211b5e76",
            "b60c47d692fa4f6d95c6c07508cbb34e",
            "e0664f2f35b3414f9fa85b818629d6f2",
            "5d36885edfc3486ea8248557c82f2ece",
            "26ce8282139f47daabbf78a46413b1f2",
            "650509c288f0450e8ca8b489de2ff2e6",
            "ca748f9ef5204a5ea3ab455483c951a9",
            "e87bc867b38f42f89fdf4639a58f7ce6",
            "dd6f010748cd4d0489a6c4fe13a16f22",
            "45a44fecb0c44335880715348f217873",
            "97e6f2cecb73485887dc96387934a9ca",
            "bd75d111fb3b41229b53090875cf2438",
            "8c75a641e4844e949f8518695d145fe1",
            "00bbcfbbcfb04ffd87b2db7024bf774e",
            "6bd763075690414eb72217575f8385ca",
            "c106d24685cd446f83e07583d6c8922d",
            "d455508db49c4eec827f80d2faf6f451",
            "c59ab837029744cbb3bec9da562d4f15",
            "3810ccb0aa9c4de5a1f3a9ae0641b61a",
            "68a0e96dc827429e8cab3a6ed1b3a7fc",
            "415d2153fe5846f4bb49038101342cb8",
            "51f0e3c5e3da4860bc9c9e52f855de59",
            "adde68fb7f1d4a59b61d56facb7a9e36",
            "b6a1081bbbe04322ae95bd561b66785f",
            "6813061815ff48aeb2bad919058b16ee",
            "7a2dba78cad64b829252c48619e2a5da",
            "8f9f3ca83fde4493b1c6bfa456917a3a",
            "3ef0530371a84737a7e1c8d5df59609b",
            "fd4acc8cb7a1492499cf15006477544b",
            "e30385d8381b4a02bd91b3f7d0e2c277",
            "bc5714edea4747e193a66aed7ba590c5",
            "b04cf1110f544da3b6421043f1e92aba",
            "b84767cde5184af08edc17f56fbb07fd",
            "eeadb21f1c904f529ebc901a8c3d324e",
            "20661349f99345aeb13c42f4ce55f53a",
            "f69cd437032f47af910fee6210dd6582",
            "8566295335704aa7bfe6dc1aabbb8abe",
            "e726e7938cc048188be6c2d74c85484f",
            "c43fcf4b710d4d149c109ef6b55a0ff8",
            "bf8043dbb00a41dd804c19fc2b4d0001",
            "cd7016a6a19c4a6db5d2e2b1a726ec9e",
            "70e3455b21594f89a751a227a38f8498",
            "a73499bf912749638d37c258fdc2720e",
            "61305f0995eb433dbbb77ccb8fb8ed24",
            "bcb65aafe409429f8bb92347c26336f7",
            "9d16f2b238c445f18bff6050026fe33d",
            "2835360b19e0435e8da2ec671e3e6d9b",
            "bea41ad9e0a640fab265fce724ee2294",
            "ccf21075549346baac5db4b095c7d83f",
            "0e6694e71ff2425ca09e1230e98f6f76",
            "3854f3790335469bad196cc961b9bdcb",
            "9f6eb7f961c942c5a91d764124fa3e96",
            "d4fcb3a89f894be0a7ba7dd688ae7218"
          ]
        },
        "outputId": "224ad09c-e504-40be-ce43-d16ce14c7314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c2c22feca8545769f9b4b99addf7933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "928068331fa4435fbba48f22df63b5af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45a44fecb0c44335880715348f217873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "415d2153fe5846f4bb49038101342cb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b04cf1110f544da3b6421043f1e92aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a73499bf912749638d37c258fdc2720e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is ML?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "8dVz4JGVBQTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "69a038f5-185e-4101-8ed2-78640e0b0202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '312307', 'source': 'Wikipedia', 'title': 'Standard ML'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Standard ML is a functional programming language which is a dialect of ML (programming language). It is sometimes used for writing compilers and in theorem provers. Here is an example of a factorial function written in a simple, non-tail recursive, style."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a neural network?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "dFtoRwndBZuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "2046a245-f408-4313-e3c5-98d42eff9c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '779314', 'source': 'Wikipedia', 'title': 'Binary Neural Network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Binary neural network is an artificial neural network, where commonly used floating-point weights are replaced with binary ones. It largely saving the storage and computation, serves as a technique for deploying deep models on resource-limited devices. Usage of binary values can bring up to 58 times speedup, while accuracy and information capacity of binary neural network can be manually controlled. Binary neural networks do not achieve the same accuracy as their full-precision counterparts, but improvements are being made to close this gap."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20151203014807Z', 'creator': 'LaTeX with hyperref package', 'file_path': './rag_docs/cnn_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20151203014807Z', 'page': 0, 'producer': 'pdfTeX-1.40.12', 'source': './rag_docs/cnn_paper.pdf', 'subject': '', 'title': '', 'total_pages': 11, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a transformer model?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "9nKGhybhZVQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "79151e0a-3469-4df5-afe6-25a17b397440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the limitations of recurrent neural networks and the advantages of attention mechanisms in sequence modeling and transduction tasks. It introduces the Transformer architecture, which eliminates recurrence and leverages attention to enhance parallelization and improve translation quality. The discussion highlights the challenges of sequential computation and the need for more efficient models in natural language processing.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the Transformer model architecture, which relies entirely on self-attention mechanisms for computing input and output representations, eliminating the need for recurrent neural networks (RNNs) or convolutions. It outlines the encoder-decoder structure commonly used in competitive neural sequence transduction models, detailing how the encoder processes input sequences into continuous representations and how the decoder generates output sequences in an auto-regressive manner.\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20230803000729Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/attention_paper.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230803000729Z', 'page': 2, 'producer': 'pdfTeX-1.40.25', 'source': './rag_docs/attention_paper.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Describes the architecture of the Transformer model, detailing the structure of the encoder and decoder stacks, which consist of multiple layers incorporating self-attention mechanisms and feed-forward networks. It explains the use of residual connections and layer normalization to enhance model performance. Additionally, it introduces the attention function, which is fundamental to the model's operation, allowing for effective mapping of queries to key-value pairs.\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. W"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is nlp?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "LCV6bZN8Zkn5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e266e9b1-ce64-43c5-b4fb-12514ab27ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '40613', 'source': 'Wikipedia', 'title': 'Natural language processing'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field in Artificial Intelligence, and is also related to linguistics. On a high level, the goal of NLP is to program computers to automatically understand human languages, and also to automatically write/speak in human languages. We say \"Natural Language\" to mean human language, and to indicate that we are not talking about computer (programming) languages."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is clustering?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "qLuSVax9ZnGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "6f8258d8-f2af-4b1f-ec0e-3b552ac92618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '593732', 'source': 'Wikipedia', 'title': 'Cluster analysis'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a vision transformer\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "ONMSpCItZovi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "ece1d079-7c3c-496e-8094-5d3d82237739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents evidence that a pure Transformer can achieve competitive performance on various image recognition benchmarks when pre-trained on large datasets.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 2, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁ"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 1, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Vision Transformer (ViT) in comparison to convolutional neural networks (CNNs), highlighting how large-scale training on extensive datasets enhances its generalization capabilities. It discusses the results achieved by ViT when pre-trained on datasets like ImageNet-21k and JFT-300M, demonstrating its competitive accuracy on various image recognition benchmarks. Additionally, it references related work that explores the integration of self-attention mechanisms in image processing.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. W"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is statistics\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "RKnF_Zbsbx8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "aa8fb01c-08c9-40ab-feba-4cc4e690b30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '789', 'source': 'Wikipedia', 'title': 'Statistics'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Statistics is a branch of applied mathematics dealing with data collection, organization, analysis, interpretation and presentation. Descriptive statistics summarize data. Inferential statistics make predictions. Statistics helps in the study of many other fields, such as science, medicine, economics, psychology, politics and marketing. Someone who works in statistics is called a statistician. In addition to being the name of a field of study, the word \"statistics\" also refers to numbers that are used to describe data or relationships. The first known statistics are census data. The Babylonians did a census around 3500 BC, the Egyptians around 2500 BC, and the Ancient Chinese around 1000 BC. Starting in the 16th century mathematicians such as Gerolamo Cardano developed probability theory, which made statistics a science. Since then, people have collected and studied statistics on many things. Trees, starfish, stars, rocks, words, almost anything that can be counted has been a subject o"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '208393', 'source': 'Wikipedia', 'title': 'Descriptive statistics'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Descriptive statistics is a branch of statistics. Its aim is to summarize a set of statistical data. The data are usually taken by sampling a population. To picture the way the data are distributed, a histogram may be drawn. The data may be summarized by computing some characterising values, like the \"center\" of the dat, and the \"spread\". In some cases the different items of data will be grouped and the groups will be described, in some way."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '470230', 'source': 'Wikipedia', 'title': 'Statistical significance'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Statistics uses variables to describe a measurement. Such a variable is called statistically significant if under a certain status quo assumption, the probability of obtaining its outcome (or a more extreme outcome) is less than a given value. Statistical significance is hence a way of determining the \"unlikeliness\" of an experimental result—when a certain status quo assumption is assumed to be true. Statistical hypothesis tests are used to check significance. The concept of statistical significance was originated by Ronald Fisher in his 1925 publication, \"Statistical Methods for Research Workers\", when he developed statistical hypothesis testing (which he described as \"tests of significance\"). Fisher suggested a probability of one in twenty (0.05 or 5%)—as a convenient cutoff level to reject the null hypothesis. In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (for example 0.05), which they called α, be set before any data collection. Despite "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}